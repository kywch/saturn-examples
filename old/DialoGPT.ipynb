{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb73be7",
   "metadata": {},
   "source": [
    "# DialoGPT from Microsoft\n",
    "\n",
    "**DialoGPT needs pytorch and transformers.**\n",
    "\n",
    "https://github.com/microsoft/DialoGPT\n",
    "https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "A State-of-the-Art Large-scale Pretrained Response generation model (DialoGPT)\n",
    "DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\n",
    "\n",
    "## Interesting links\n",
    "* https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG A great tutorial of how to finetune DialoGPT to build a customized bot built by Rostyslav Neskorozhenyi. \n",
    "* If we can fine tune, something like Children's Book Test (https://github.com/facebookarchive/bAbI-tasks) could be useful. \n",
    "* Interesting conversation data set: My Science Tutor Children's Speech Corpus (https://boulderlearning.com/request-the-myst-corpus/)\n",
    "\n",
    "\n",
    "Let's make a chatbot! Source: https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n",
    "\n",
    "## To download the pre-trained models, uncomment and run the below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4855800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pretrained DialoGPT-medium is ~1G\n",
    "# the pretrained DialoGPT-large is ~2G\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: how's the weather\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: Cloudy, but nice and sunny.\n",
      "DialoGPT 1: It's pretty hot and sunny atm, but the weather isn't too bad.\n",
      "DialoGPT 2: It's amazing\n",
      "DialoGPT 3: Haven't experienced any yet\n",
      "DialoGPT 4: It's a bit cloudy. I'm on the southern coast so it is pretty rainy.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  4\n",
      ">> You: i don't like the rain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: It's just raining here.\n",
      "DialoGPT 1: Not much rain here. Just some light rain.\n",
      "DialoGPT 2: It's okay. I'm used to it.\n",
      "DialoGPT 3: It's not raining. I just get lazy with rain.\n",
      "DialoGPT 4: It's not rain, it's the clouds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  2\n",
      ">> You: you are weird\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: I try.\n",
      "DialoGPT 1: I'm not.\n",
      "DialoGPT 2: you arent\n",
      "DialoGPT 3: I'm a weirdo.\n",
      "DialoGPT 4: How so?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  0\n",
      ">> You: you and I won't make a good friend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: You are weird.\n",
      "DialoGPT 1: oh ok then\n",
      "DialoGPT 2: oh I know\n",
      "DialoGPT 3: You might be right.\n",
      "DialoGPT 4: That's what we're saying\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  0\n",
      ">> You: i'm hurt you are so mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: me too\n",
      "DialoGPT 1: I mean.\n",
      "DialoGPT 2: I know you're hurt.\n",
      "DialoGPT 3: That's okay.\n",
      "DialoGPT 4: you are not that nice\n"
     ]
    }
   ],
   "source": [
    "# chatting 5 times with nucleus & top-k sampling & tweaking temperature & multiple sentences\n",
    "\n",
    "for step in range(5):\n",
    "\n",
    "    # take user input\n",
    "    text = input(\">> You:\")\n",
    "    \n",
    "    # encode the input and add end of string token\n",
    "    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    \n",
    "    # concatenate new user input with chat history (if there is)\n",
    "    bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n",
    "    \n",
    "    # generate a bot response\n",
    "    chat_history_ids_list = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1000,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.75,\n",
    "        num_return_sequences=5,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    #print the outputs\n",
    "    for i in range(len(chat_history_ids_list)):\n",
    "      output = tokenizer.decode(chat_history_ids_list[i][bot_input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "      print(f\"DialoGPT {i}: {output}\")\n",
    "    \n",
    "    choice_index = int(input(\"Choose the response you want for the next input: \"))\n",
    "    \n",
    "    chat_history_ids = torch.unsqueeze(chat_history_ids_list[choice_index], dim=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea073e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
