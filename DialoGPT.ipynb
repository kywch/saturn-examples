{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51db104",
   "metadata": {},
   "source": [
    "# DialoGPT from Microsoft\n",
    "\n",
    "**DialoGPT needs pytorch and transformers.**\n",
    "\n",
    "https://github.com/microsoft/DialoGPT\n",
    "https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "A State-of-the-Art Large-scale Pretrained Response generation model (DialoGPT)\n",
    "DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test. The model is trained on 147M multi-turn dialogue from Reddit discussion thread.\n",
    "\n",
    "## Interesting use cases\n",
    "* https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG A great tutorial of how to finetune DialoGPT to build a customized bot built by Rostyslav Neskorozhenyi. \n",
    "\n",
    "Let's make a chatbot! Source: https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python\n",
    "\n",
    "## To download the pre-trained models, uncomment and run the below lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c205a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pretrained DialoGPT-medium is ~1G\n",
    "# the pretrained DialoGPT-large is ~2G\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745f4dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: what's your favorite food?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: I'm a big fan of French toast.\n",
      "DialoGPT 1: Haven't had it in a while. I like sushi and sushi is my favorite.\n",
      "DialoGPT 2: I like pizza\n",
      "DialoGPT 3: I like food.\n",
      "DialoGPT 4: chicken parm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  1\n",
      ">> You: I also love sushi. could you recommend any restaurants?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: I like The Sushi Club on 3rd and 11th. It's just a little bit out of my way, but it's not bad.\n",
      "DialoGPT 1: I think they're all over the place.\n",
      "DialoGPT 2: I don't like restaurants. I like sushi food.\n",
      "DialoGPT 3: I like Nijiro. They have awesome sushi.\n",
      "DialoGPT 4: I like the sushi at Haruki Nakamura in Nihonjin, it's a bit pricey, but it's great.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  4\n",
      ">> You: Oh I want to go there. How much is it?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: It's a little pricey but it's a great restaurant.\n",
      "DialoGPT 1: 25 for 4 meals.\n",
      "DialoGPT 2: 75 for the meal.\n",
      "DialoGPT 3: It's about 4.5 or 5 dollars. It's near a train station.\n",
      "DialoGPT 4: It's 20\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  2\n",
      ">> You: hmm not too bad. I can buy you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: yeah. But\n",
      "DialoGPT 1: hahahaha\n",
      "DialoGPT 2: Haha\n",
      "DialoGPT 3: It's on my wish list\n",
      "DialoGPT 4: with a free meal, too.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  1\n",
      ">> You: hahahaha\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT 0: hahahaha\n",
      "DialoGPT 1: this guy is a real\n",
      "DialoGPT 2: oh that's a good\n",
      "DialoGPT 3: I'm not sure but\n",
      "DialoGPT 4: lol what a guy.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose the response you want for the next input:  4\n"
     ]
    }
   ],
   "source": [
    "# chatting 5 times with nucleus & top-k sampling & tweaking temperature & multiple sentences\n",
    "\n",
    "for step in range(5):\n",
    "\n",
    "    # take user input\n",
    "    text = input(\">> You:\")\n",
    "    \n",
    "    # encode the input and add end of string token\n",
    "    input_ids = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    \n",
    "    # concatenate new user input with chat history (if there is)\n",
    "    bot_input_ids = torch.cat([chat_history_ids, input_ids], dim=-1) if step > 0 else input_ids\n",
    "    \n",
    "    # generate a bot response\n",
    "    chat_history_ids_list = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=1000,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=50,\n",
    "        temperature=0.75,\n",
    "        num_return_sequences=5,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    #print the outputs\n",
    "    for i in range(len(chat_history_ids_list)):\n",
    "      output = tokenizer.decode(chat_history_ids_list[i][bot_input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "      print(f\"DialoGPT {i}: {output}\")\n",
    "    \n",
    "    choice_index = int(input(\"Choose the response you want for the next input: \"))\n",
    "    \n",
    "    chat_history_ids = torch.unsqueeze(chat_history_ids_list[choice_index], dim=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f2c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
