{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost regression (multi-node with Dask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/dask.png\" width=\"425\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/xgboost.png\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll load data into a [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) and use [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) to answer this question\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, what tip will this trip earn (as a % of the total fare)?\n",
    "\n",
    "This notebook gives an introductory tutorial on how to use Dask to scale training of XGBoost models. For more detailed information, see [\"Distributed XGBoost with Dask\"](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) in the XGBoost documentation and [\"XGBoost Training with Dask\"](https://www.saturncloud.io/docs/tutorials/xgboost/) in Saturn Cloud's documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Initialize A Dask Cluster\n",
    "\n",
    "This tutorial uses multiple machines to show how to apply more computing resources to machine learning training. This is done with Dask. Saturn Cloud offers managed Dask clusters, which can be provisioned and modified programmatically.\n",
    "\n",
    "The code below creates a Dask cluster using [`dask-saturn`](https://github.com/saturncloud/dask-saturn), the official Dask client for Saturn Cloud. It creates a cluster with the following specs:\n",
    "\n",
    "* `n_workers=3` --> 3 machines in the cluster\n",
    "* `scheduler_size='medium'` --> the Dask scheduler will have 4GB of RAM and 2 CPU cores\n",
    "* `worker_size='large'` --> each worker machine will have 2 CPU cores and 16GB of RAM\n",
    "\n",
    "To see a list of possible sizes, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.distributed import Client, wait\n",
    "#from dask_saturn import SaturnCluster\n",
    "\n",
    "#n_workers = 3\n",
    "#cluster = SaturnCluster(n_workers=n_workers, scheduler_size=\"medium\", worker_size=\"large\")\n",
    "#client = Client(cluster)\n",
    "\n",
    "from dask.distributed import Client, LocalCluster, wait\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster in the Saturn UI if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers() #n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Resource Usage\n",
    "\n",
    "This tutorial aims to teach you how to take advantage of multiple machines for data science workflows. To prove to yourself that Dask is taking advantage of the resources in the cluster, it's important to understand how to monitor that utilization while your code is running.\n",
    "\n",
    "Print the `cluster` object in a notebook renders a widget that shows the number of workers, available CPU and memory, and a dashboard link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d36f79bda044ac094bb3a64001e1dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click that dashboard link to view some diagnostic information about the Dask cluster. This can be used to view the current resource utilization of workers in the cluster and lots of information about what they're currently working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example is designed to run quickly with small, relatively inexpensive resources. So let's just load a single month of taxi data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 784 ms, sys: 112 ms, total: 896 ms\n",
      "Wall time: 877 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "taxi = dd.read_csv(\n",
    "    \"yellow_tripdata_2019-01.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": True},\n",
    "    assume_missing=True,\n",
    ") #.sample(frac=0.3, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the size of this dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 7667792, Size: 1487.553056 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the structure of the data with Dask DataFrame commands:\n",
    "\n",
    "`.head()` = view the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:46:40</td>\n",
       "      <td>2019-01-01 00:53:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>151.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-01-01 00:59:47</td>\n",
       "      <td>2019-01-01 01:18:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-12-21 13:48:30</td>\n",
       "      <td>2018-12-21 13:52:40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 15:52:25</td>\n",
       "      <td>2018-11-28 15:55:45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2018-11-28 15:56:57</td>\n",
       "      <td>2018-11-28 15:58:33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>55.55</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       1.0  2019-01-01 00:46:40   2019-01-01 00:53:20              1.0   \n",
       "1       1.0  2019-01-01 00:59:47   2019-01-01 01:18:59              1.0   \n",
       "2       2.0  2018-12-21 13:48:30   2018-12-21 13:52:40              3.0   \n",
       "3       2.0  2018-11-28 15:52:25   2018-11-28 15:55:45              5.0   \n",
       "4       2.0  2018-11-28 15:56:57   2018-11-28 15:58:33              5.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.5         1.0                  N         151.0         239.0   \n",
       "1            2.6         1.0                  N         239.0         246.0   \n",
       "2            0.0         1.0                  N         236.0         236.0   \n",
       "3            0.0         1.0                  N         193.0         193.0   \n",
       "4            0.0         2.0                  N         193.0         193.0   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0           1.0          7.0    0.5      0.5        1.65           0.0   \n",
       "1           1.0         14.0    0.5      0.5        1.00           0.0   \n",
       "2           1.0          4.5    0.5      0.5        0.00           0.0   \n",
       "3           2.0          3.5    0.5      0.5        0.00           0.0   \n",
       "4           2.0         52.0    0.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3          9.95                   NaN  \n",
       "1                    0.3         16.30                   NaN  \n",
       "2                    0.3          5.80                   NaN  \n",
       "3                    0.3          7.55                   NaN  \n",
       "4                    0.3         55.55                   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dtypes` = list all the columns and the type of data in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                        float64\n",
       "tpep_pickup_datetime     datetime64[ns]\n",
       "tpep_dropoff_datetime    datetime64[ns]\n",
       "passenger_count                 float64\n",
       "trip_distance                   float64\n",
       "RatecodeID                      float64\n",
       "store_and_fwd_flag               object\n",
       "PULocationID                    float64\n",
       "DOLocationID                    float64\n",
       "payment_type                    float64\n",
       "fare_amount                     float64\n",
       "extra                           float64\n",
       "mta_tax                         float64\n",
       "tip_amount                      float64\n",
       "tolls_amount                    float64\n",
       "improvement_surcharge           float64\n",
       "total_amount                    float64\n",
       "congestion_surcharge            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Prep for Training\n",
    "\n",
    "Before training a model, we need to transform this dataset into a format that's better-suited to the research question. The function below does that with Dask DataFrame operations.\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Compute the Target**\n",
    "\n",
    "The raw data don't contain a column that cleanly describes the tip as a percentage of the total fare. So we need to add one!\n",
    "\n",
    "**Add Features**\n",
    "\n",
    "Giving a machine learning model a richer description of each training observation improves its ability to describe the relationship between those observations' characteristics and the target. These characteristics are called \"features\".\n",
    "\n",
    "For example, instead of giving a model a raw timestamp, it can be valuable to provide multiple derived characteristics like hour of the day and day of the week. It's plausible, for example, that weekend rides might have a different distribution of tips because they tend to be for leisure, where weekday rides might be mostly people travelling for work.\n",
    "\n",
    "**Remove Unused Features**\n",
    "\n",
    "If the goal is to produce a model that could predict the tip for a ride, then characteristics that can only be known AFTER the tip have to be excluded. For example, you can't know the dropoff time or the type of payment until a ride has concluded.\n",
    "\n",
    "Such features should be dropped before training.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df: dd.DataFrame, target_col: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a raw taxi dataframe for training.\n",
    "        * computes the target ('tip_fraction')\n",
    "        * adds features\n",
    "        * removes unused features\n",
    "    \"\"\"\n",
    "    numeric_feat = [\n",
    "        \"pickup_weekday\",\n",
    "        \"pickup_weekofyear\",\n",
    "        \"pickup_hour\",\n",
    "        \"pickup_week_hour\",\n",
    "        \"pickup_minute\",\n",
    "        \"passenger_count\",\n",
    "    ]\n",
    "    categorical_feat = [\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "    ]\n",
    "    features = numeric_feat + categorical_feat\n",
    "    df = df[df.fare_amount > 0]  # avoid divide-by-zero\n",
    "    df[target_col] = df.tip_amount / df.fare_amount\n",
    "\n",
    "    df[\"pickup_weekday\"] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df[\"pickup_weekofyear\"] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df[\"pickup_hour\"] = df.tpep_pickup_datetime.dt.hour\n",
    "    df[\"pickup_week_hour\"] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df[\"pickup_minute\"] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [target_col]].astype(float).fillna(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to get a new data frame, `taxi_train`, that can be used directly for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"tip_fraction\"\n",
    "taxi_train = prep_df(taxi, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask performs computations in a [lazy manner](https://tutorial.dask.org/01x_lazy.html), so we persist the dataframe to perform data loading and feature processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 288 ms, sys: 36 ms, total: 324 ms\n",
      "Wall time: 4.93 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_weekday</th>\n",
       "      <th>pickup_weekofyear</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_week_hour</th>\n",
       "      <th>pickup_minute</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>tip_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.235714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_weekday  pickup_weekofyear  pickup_hour  pickup_week_hour  \\\n",
       "0             1.0                1.0          0.0              24.0   \n",
       "1             1.0                1.0          0.0              24.0   \n",
       "2             4.0               51.0         13.0             109.0   \n",
       "3             2.0               48.0         15.0              63.0   \n",
       "4             2.0               48.0         15.0              63.0   \n",
       "\n",
       "   pickup_minute  passenger_count  PULocationID  DOLocationID  tip_fraction  \n",
       "0           46.0              1.0         151.0         239.0      0.235714  \n",
       "1           59.0              1.0         239.0         246.0      0.071429  \n",
       "2           48.0              3.0         236.0         236.0      0.000000  \n",
       "3           52.0              5.0         193.0         193.0      0.000000  \n",
       "4           56.0              5.0         193.0         193.0      0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "taxi_train = taxi_train.persist()\n",
    "_ = wait(taxi_train)\n",
    "\n",
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, check the first few rows of the dataset to make sure that the features look reasonable.\n",
    "\n",
    "Now that the dataframe has been processed, check its size in memory again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 7658235, Size: 0.6126588 GB\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Num rows: {len(taxi_train)}, Size: {taxi_train.memory_usage(deep=True).sum().compute() / 1e9} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "This example uses the native Dask integration built into XGBoost. That integration was added in `xgboost` 1.3.0, and should be preferred to [`dask-xgboost`](https://github.com/dask/dask-xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data for `xgboost.dask` needs to be prepared in a special object called `DaskDMatrix`. This is like the XGBoost `DMatrix` that you might be familiar with, but is backed by Dask's distributed collections (Dask DataFrame and Dask Array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in taxi_train.columns if c != target_col]\n",
    "\n",
    "data = taxi_train[features]\n",
    "label = taxi_train[target_col]\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client=client, data=data, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any [xgboost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) to `xgb.dask.train()`. The training process will then start up on all workers that have some of the data in `dtrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 516 ms, sys: 80 ms, total: 596 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = xgb.dask.train(\n",
    "    client=client,\n",
    "    params={\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 5,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Save model\n",
    "\n",
    "`xgb.dask.train()` produces a regular `xgb.core.Booster` object, the same model object produced by non-Dask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xgboost.core.Booster"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster = result[\"booster\"]\n",
    "type(booster)\n",
    "\n",
    "# xgboost.core.Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've trained a model, save it in a file to use later for scoring or for comparison with other models.\n",
    "\n",
    "There are several ways to do this, but `cloudpickle` is likely to give you the best experience. It handles some common drawbacks of the built-in `pickle` library.\n",
    "\n",
    "`cloudpickle` can be used to write a Python object to bytes, and to create a Python object from that binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/xgboost_dask.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(booster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = dd.read_csv(\n",
    "    \"yellow_tripdata_2019-02.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": True},\n",
    "    assume_missing=True,\n",
    ") #.sample(frac=0.01, replace=False)\n",
    "\n",
    "taxi_test = prep_df(taxi_test, target_col=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgboost.dask.predict()` can be used to create predictions on a Dask collection using an XGBoost model object. Because the model object here is just a regular XGBoost model, using `xgboost.dask` for batch scoring doesn't require that you also perform training on Dask.\n",
    "\n",
    "This function returns a Dask Array or Dask Series of predictions, depending on the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.dask.predict(client=client, model=booster, data=taxi_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics functions in `dask_ml` can compute metrics on Dask collections like Dask Array and Dask DataFrame, so you never have to hold all of the test data in memory on the client. These functions intentionally mimic the metrics functions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.314497211968504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(taxi_test[target_col].to_dask_array(), preds.to_dask_array(), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use Dask and `xgboost` for distributed model training.\n",
    "\n",
    "If you want to try working with larger datasets, change the code in this notebook to use more data or explore how to programmatically resize your Dask cluster by following the tips in [\"Managing Dask Resources for Machine Learning Training\"](https://www.saturncloud.io/docs/tips-and-tricks/training-on-dask).\n",
    "\n",
    "Alternatively, try [this notebook](./dashboard.ipynb) to learn how to use Saturn Cloud to deploy an interactive dashboard that includes a component which uses the models trained above.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
